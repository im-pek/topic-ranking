import bs4 as bs
import urllib.request
import re
import nltk
from nltk.tokenize import sent_tokenize
import os

with open("to rank titles (comfort women, MA).txt") as file:
    
    data = file.read()

    entries=data.split("\n")
    
    sentenced = [item+'. ' for item in entries]
    
    article=''.join(sentenced)
        
    parsed_article = bs.BeautifulSoup(article,'lxml')
    
    paragraphs = parsed_article.find_all('p')
    
    article_text = ""
    
    for p in paragraphs:  
        article_text += p.text
    
    sentence_list = nltk.sent_tokenize(article_text)
    
    word_frequencies = {}

    for word in sentence_list:
        if word not in word_frequencies.keys():
            word_frequencies[word] = 1
        
        else:
            word_frequencies[word] += 1
                
    maximum_frequncy = max(word_frequencies.values())
    
    for word in word_frequencies.keys():  
        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)
    
    sentence_scores = {}  
    for sent in sentence_list:  
        for word in nltk.word_tokenize(sent.lower()):
            if word in word_frequencies.keys():
                if len(sent.split(' ')) < 30:
                    if sent not in sentence_scores.keys():
                        sentence_scores[sent] = word_frequencies[word]
                    else:
                        sentence_scores[sent] += word_frequencies[word]
    print (sentence_scores)
                   
    import heapq
    summary_sentences = heapq.nlargest(133, sentence_scores, key=sentence_scores.get)

#    print (summary_sentences)

with open('ranked titles comfort women MA.txt', 'w') as ff:
    for line in summary_sentences:
        ff.write(line)
        ff.write('\n')
